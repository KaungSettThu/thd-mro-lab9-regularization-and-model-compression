{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hqc7VzRT8oEx"
   },
   "source": [
    "## Lab09.2 Model Compression\n",
    "\n",
    "Select free T4 GPU in Runtime settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vr4RoazRb0vf"
   },
   "source": [
    "## Download data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 25102,
     "status": "ok",
     "timestamp": 1748244125489,
     "user": {
      "displayName": "Tobias Schaffer",
      "userId": "09009542802022320095"
     },
     "user_tz": -120
    },
    "id": "2U0FsOf64jxP",
    "outputId": "51813cbe-f015-444d-f27e-47400811c9a2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From (original): https://drive.google.com/uc?id=12WhCCpKTWpeBztLegcoYx2gMo2KbaxDG\n",
      "From (redirected): https://drive.google.com/uc?id=12WhCCpKTWpeBztLegcoYx2gMo2KbaxDG&confirm=t&uuid=ddd0b742-7162-44c7-8d2a-d7ceabc71cc1\n",
      "To: /home/kast/m-eng-robotics/embeded-systems/thd-mro-em-labs/lab-9/dogs-vs-cats.zip\n",
      "100%|████████████████████████████████████████| 852M/852M [00:58<00:00, 14.7MB/s]\n"
     ]
    }
   ],
   "source": [
    "!pip install -q gdown\n",
    "!gdown https://drive.google.com/uc?id=12WhCCpKTWpeBztLegcoYx2gMo2KbaxDG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "-VoiZIDhTo4i"
   },
   "outputs": [],
   "source": [
    "import zipfile\n",
    "\n",
    "with zipfile.ZipFile('dogs-vs-cats.zip', 'r') as zip_file:\n",
    "    zip_file.extractall('data')\n",
    "\n",
    "with zipfile.ZipFile('data/train.zip', 'r') as zip_file:\n",
    "    zip_file.extractall('data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "EcfuxxduUHwB"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Paths\n",
    "base_dir = 'data/train'\n",
    "train_dir = 'data/train_split'\n",
    "val_dir = 'data/val_split'\n",
    "\n",
    "# Create directories\n",
    "os.makedirs(os.path.join(train_dir, 'dogs'), exist_ok=True)\n",
    "os.makedirs(os.path.join(train_dir, 'cats'), exist_ok=True)\n",
    "os.makedirs(os.path.join(val_dir, 'dogs'), exist_ok=True)\n",
    "os.makedirs(os.path.join(val_dir, 'cats'), exist_ok=True)\n",
    "\n",
    "# Split data\n",
    "filenames = os.listdir(base_dir)\n",
    "train_files, val_files = train_test_split(filenames, test_size=0.2, random_state=42)\n",
    "\n",
    "for file in train_files:\n",
    "    if 'dog' in file:\n",
    "        shutil.move(os.path.join(base_dir, file), os.path.join(train_dir, 'dogs', file))\n",
    "    elif 'cat' in file:\n",
    "        shutil.move(os.path.join(base_dir, file), os.path.join(train_dir, 'cats', file))\n",
    "\n",
    "for file in val_files:\n",
    "    if 'dog' in file:\n",
    "        shutil.move(os.path.join(base_dir, file), os.path.join(val_dir, 'dogs', file))\n",
    "    elif 'cat' in file:\n",
    "        shutil.move(os.path.join(base_dir, file), os.path.join(val_dir, 'cats', file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K5S0ZsZ_cFD8"
   },
   "source": [
    "## Create data generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 437,
     "status": "ok",
     "timestamp": 1748244162917,
     "user": {
      "displayName": "Tobias Schaffer",
      "userId": "09009542802022320095"
     },
     "user_tz": -120
    },
    "id": "vZVXg3GxUZ4D",
    "outputId": "7d4af5cf-cb58-420f-ee75-41a28c090c20"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 20000 images belonging to 2 classes.\n",
      "Found 5000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "val_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# Load data from directories\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(150, 150),\n",
    "    batch_size=32,\n",
    "    class_mode='binary'\n",
    ")\n",
    "\n",
    "val_generator = val_datagen.flow_from_directory(\n",
    "    val_dir,\n",
    "    target_size=(150, 150),\n",
    "    batch_size=32,\n",
    "    class_mode='binary'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kwQWFFtu_ClZ"
   },
   "source": [
    "## Create basic CNN classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "08cAfSIlT23T"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import models, layers\n",
    "\n",
    "# Build the model\n",
    "model_basic = models.Sequential([\n",
    "    layers.Input(shape=(150, 150, 3)),\n",
    "\n",
    "    layers.Conv2D(32, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "\n",
    "    layers.Conv2D(128, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    \n",
    "    layers.Flatten(),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model_basic.compile(\n",
    "    optimizer='adam',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RLpHLQqS_HZf"
   },
   "source": [
    "## Train basic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 328922,
     "status": "ok",
     "timestamp": 1748244493953,
     "user": {
      "displayName": "Tobias Schaffer",
      "userId": "09009542802022320095"
     },
     "user_tz": -120
    },
    "id": "GjXNl6F0U_4R",
    "outputId": "fefba9c1-70b2-4f42-e7df-1bd338e55a28"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kast/m-eng-robotics/embeded-systems/thd-mro-em-labs/.venv/lib/python3.10/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m242s\u001b[0m 377ms/step - accuracy: 0.6033 - loss: 0.6631 - val_accuracy: 0.7508 - val_loss: 0.5133\n",
      "Epoch 2/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m207s\u001b[0m 331ms/step - accuracy: 0.7674 - loss: 0.4824 - val_accuracy: 0.8030 - val_loss: 0.4245\n",
      "Epoch 3/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m213s\u001b[0m 340ms/step - accuracy: 0.8206 - loss: 0.3898 - val_accuracy: 0.8294 - val_loss: 0.3795\n",
      "Epoch 4/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m212s\u001b[0m 339ms/step - accuracy: 0.8620 - loss: 0.3149 - val_accuracy: 0.8268 - val_loss: 0.4146\n",
      "Epoch 5/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m212s\u001b[0m 338ms/step - accuracy: 0.9095 - loss: 0.2154 - val_accuracy: 0.8428 - val_loss: 0.4161\n",
      "Epoch 6/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m215s\u001b[0m 343ms/step - accuracy: 0.9519 - loss: 0.1224 - val_accuracy: 0.8414 - val_loss: 0.5162\n",
      "Epoch 7/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m216s\u001b[0m 345ms/step - accuracy: 0.9812 - loss: 0.0581 - val_accuracy: 0.7870 - val_loss: 0.6783\n",
      "Epoch 8/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m217s\u001b[0m 348ms/step - accuracy: 0.9875 - loss: 0.0403 - val_accuracy: 0.8382 - val_loss: 0.7664\n",
      "Epoch 9/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m219s\u001b[0m 350ms/step - accuracy: 0.9914 - loss: 0.0268 - val_accuracy: 0.8310 - val_loss: 0.7897\n",
      "Epoch 10/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m220s\u001b[0m 352ms/step - accuracy: 0.9901 - loss: 0.0374 - val_accuracy: 0.8326 - val_loss: 0.9135\n"
     ]
    }
   ],
   "source": [
    "history_model_basic = model_basic.fit(\n",
    "    train_generator,\n",
    "    epochs=10,\n",
    "    validation_data=val_generator,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bfa-2S5bUFAx"
   },
   "source": [
    "# Tasks\n",
    "\n",
    "## Save model and compress\n",
    "\n",
    "Refer to the provided lab instructions document 'Lab09.2 Model Compression'\n",
    "\n",
    "- Save the full model to disk (*.keras format).\n",
    "- Convert the model to TFLite using the following methods:\n",
    "    - Default conversion\n",
    "    - Weight quantization (weights to INT8)\n",
    "    - Float16 quantization (weights to FP16)\n",
    "    - Integer quantization (weights and activations to INT8, use a representative dataset for calibration)\n",
    "- Compare the file sizes of the different models.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.makedirs(\"models\", exist_ok=True)\n",
    "\n",
    "# Save the model as keras\n",
    "model_basic.save(\"models/model_basic.keras\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model file size: 55.30 MB\n"
     ]
    }
   ],
   "source": [
    "# Get size in bytes and convert to megabytes\n",
    "file_size_mb = os.path.getsize(\"models/model_basic.keras\") / (1024 * 1024)\n",
    "print(f\"Model file size: {file_size_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "model = tf.keras.models.load_model(\"models/model_basic.keras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Default TFLite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpknajm63v/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpknajm63v/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved artifact at '/tmp/tmpknajm63v'. The following endpoints are available:\n",
      "\n",
      "* Endpoint 'serve'\n",
      "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 150, 150, 3), dtype=tf.float32, name='input_layer_1')\n",
      "Output Type:\n",
      "  TensorSpec(shape=(None, 1), dtype=tf.float32, name=None)\n",
      "Captures:\n",
      "  135307042878944: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  135308579776368: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  135306911166208: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  135306911168848: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  135306911165328: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  135306913141984: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  135306913143040: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  135306913076096: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  135306913078560: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  135306913076624: TensorSpec(shape=(), dtype=tf.resource, name=None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1750348309.776299  932618 tf_tfl_flatbuffer_helpers.cc:365] Ignored output_format.\n",
      "W0000 00:00:1750348309.776564  932618 tf_tfl_flatbuffer_helpers.cc:368] Ignored drop_control_dependency.\n",
      "2025-06-19 17:51:49.778863: I tensorflow/cc/saved_model/reader.cc:83] Reading SavedModel from: /tmp/tmpknajm63v\n",
      "2025-06-19 17:51:49.779708: I tensorflow/cc/saved_model/reader.cc:52] Reading meta graph with tags { serve }\n",
      "2025-06-19 17:51:49.779724: I tensorflow/cc/saved_model/reader.cc:147] Reading SavedModel debug info (if present) from: /tmp/tmpknajm63v\n",
      "2025-06-19 17:51:49.792050: I tensorflow/cc/saved_model/loader.cc:236] Restoring SavedModel bundle.\n",
      "2025-06-19 17:51:49.925457: I tensorflow/cc/saved_model/loader.cc:220] Running initialization op on SavedModel bundle at path: /tmp/tmpknajm63v\n",
      "2025-06-19 17:51:49.947616: I tensorflow/cc/saved_model/loader.cc:471] SavedModel load for tags { serve }; Status: success: OK. Took 168522 microseconds.\n"
     ]
    }
   ],
   "source": [
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "with open(\"models/model_basic_default.tflite\", \"wb\") as f:\n",
    "    f.write(tflite_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weight Quantization\n",
    "8-bit integers (INT8) precision for the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpi4yfcfn_/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpi4yfcfn_/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved artifact at '/tmp/tmpi4yfcfn_'. The following endpoints are available:\n",
      "\n",
      "* Endpoint 'serve'\n",
      "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 150, 150, 3), dtype=tf.float32, name='input_layer_1')\n",
      "Output Type:\n",
      "  TensorSpec(shape=(None, 1), dtype=tf.float32, name=None)\n",
      "Captures:\n",
      "  135307042878944: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  135308579776368: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  135306911166208: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  135306911168848: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  135306911165328: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  135306913141984: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  135306913143040: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  135306913076096: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  135306913078560: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  135306913076624: TensorSpec(shape=(), dtype=tf.resource, name=None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1750348380.405488  932618 tf_tfl_flatbuffer_helpers.cc:365] Ignored output_format.\n",
      "W0000 00:00:1750348380.405699  932618 tf_tfl_flatbuffer_helpers.cc:368] Ignored drop_control_dependency.\n",
      "2025-06-19 17:53:00.408415: I tensorflow/cc/saved_model/reader.cc:83] Reading SavedModel from: /tmp/tmpi4yfcfn_\n",
      "2025-06-19 17:53:00.410793: I tensorflow/cc/saved_model/reader.cc:52] Reading meta graph with tags { serve }\n",
      "2025-06-19 17:53:00.410821: I tensorflow/cc/saved_model/reader.cc:147] Reading SavedModel debug info (if present) from: /tmp/tmpi4yfcfn_\n",
      "2025-06-19 17:53:00.428950: I tensorflow/cc/saved_model/loader.cc:236] Restoring SavedModel bundle.\n",
      "2025-06-19 17:53:00.570109: I tensorflow/cc/saved_model/loader.cc:220] Running initialization op on SavedModel bundle at path: /tmp/tmpi4yfcfn_\n",
      "2025-06-19 17:53:00.587111: I tensorflow/cc/saved_model/loader.cc:471] SavedModel load for tags { serve }; Status: success: OK. Took 179045 microseconds.\n"
     ]
    }
   ],
   "source": [
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "with open(\"models/model_basic_weight_quant.tflite\", \"wb\") as f:\n",
    "    f.write(tflite_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Float16 quantization\n",
    "16-bit floating-point (FP16) precision for the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmp0bhq9igf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmp0bhq9igf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved artifact at '/tmp/tmp0bhq9igf'. The following endpoints are available:\n",
      "\n",
      "* Endpoint 'serve'\n",
      "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 150, 150, 3), dtype=tf.float32, name='input_layer_1')\n",
      "Output Type:\n",
      "  TensorSpec(shape=(None, 1), dtype=tf.float32, name=None)\n",
      "Captures:\n",
      "  135307042878944: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  135308579776368: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  135306911166208: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  135306911168848: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  135306911165328: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  135306913141984: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  135306913143040: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  135306913076096: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  135306913078560: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  135306913076624: TensorSpec(shape=(), dtype=tf.resource, name=None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1750348418.638366  932618 tf_tfl_flatbuffer_helpers.cc:365] Ignored output_format.\n",
      "W0000 00:00:1750348418.638767  932618 tf_tfl_flatbuffer_helpers.cc:368] Ignored drop_control_dependency.\n",
      "2025-06-19 17:53:38.640739: I tensorflow/cc/saved_model/reader.cc:83] Reading SavedModel from: /tmp/tmp0bhq9igf\n",
      "2025-06-19 17:53:38.642583: I tensorflow/cc/saved_model/reader.cc:52] Reading meta graph with tags { serve }\n",
      "2025-06-19 17:53:38.642610: I tensorflow/cc/saved_model/reader.cc:147] Reading SavedModel debug info (if present) from: /tmp/tmp0bhq9igf\n",
      "2025-06-19 17:53:38.659512: I tensorflow/cc/saved_model/loader.cc:236] Restoring SavedModel bundle.\n",
      "2025-06-19 17:53:38.831594: I tensorflow/cc/saved_model/loader.cc:220] Running initialization op on SavedModel bundle at path: /tmp/tmp0bhq9igf\n",
      "2025-06-19 17:53:38.857026: I tensorflow/cc/saved_model/loader.cc:471] SavedModel load for tags { serve }; Status: success: OK. Took 215941 microseconds.\n"
     ]
    }
   ],
   "source": [
    "converter.target_spec.supported_types = [tf.float16]\n",
    "tflite_fp16 = converter.convert()\n",
    "\n",
    "with open(\"models/model_basic_fp16.tflite\", \"wb\") as f:\n",
    "    f.write(tflite_fp16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integer quantization\n",
    "8-bit integer (INT8) pricision for both weights and activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmp90d6q39d/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmp90d6q39d/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved artifact at '/tmp/tmp90d6q39d'. The following endpoints are available:\n",
      "\n",
      "* Endpoint 'serve'\n",
      "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 150, 150, 3), dtype=tf.float32, name='input_layer_1')\n",
      "Output Type:\n",
      "  TensorSpec(shape=(None, 1), dtype=tf.float32, name=None)\n",
      "Captures:\n",
      "  135307042878944: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  135308579776368: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  135306911166208: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  135306911168848: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  135306911165328: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  135306913141984: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  135306913143040: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  135306913076096: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  135306913078560: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  135306913076624: TensorSpec(shape=(), dtype=tf.resource, name=None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kast/m-eng-robotics/embeded-systems/thd-mro-em-labs/.venv/lib/python3.10/site-packages/tensorflow/lite/python/convert.py:854: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.\n",
      "  warnings.warn(\n",
      "W0000 00:00:1750348636.585376  932618 tf_tfl_flatbuffer_helpers.cc:365] Ignored output_format.\n",
      "W0000 00:00:1750348636.585741  932618 tf_tfl_flatbuffer_helpers.cc:368] Ignored drop_control_dependency.\n",
      "2025-06-19 17:57:16.589688: I tensorflow/cc/saved_model/reader.cc:83] Reading SavedModel from: /tmp/tmp90d6q39d\n",
      "2025-06-19 17:57:16.592066: I tensorflow/cc/saved_model/reader.cc:52] Reading meta graph with tags { serve }\n",
      "2025-06-19 17:57:16.592181: I tensorflow/cc/saved_model/reader.cc:147] Reading SavedModel debug info (if present) from: /tmp/tmp90d6q39d\n",
      "2025-06-19 17:57:16.613807: I tensorflow/cc/saved_model/loader.cc:236] Restoring SavedModel bundle.\n",
      "2025-06-19 17:57:16.943988: I tensorflow/cc/saved_model/loader.cc:220] Running initialization op on SavedModel bundle at path: /tmp/tmp90d6q39d\n",
      "2025-06-19 17:57:17.003337: I tensorflow/cc/saved_model/loader.cc:471] SavedModel load for tags { serve }; Status: success: OK. Took 413881 microseconds.\n",
      "fully_quantize: 0, inference_type: 6, input_inference_type: INT8, output_inference_type: INT8\n"
     ]
    }
   ],
   "source": [
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "\n",
    "def representative_data_gen():\n",
    "    for _ in range(100):\n",
    "        data, _ = next(train_generator)\n",
    "        yield [data.astype(\"float32\")]\n",
    "\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.representative_dataset = representative_data_gen\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "converter.inference_input_type = tf.int8\n",
    "converter.inference_output_type = tf.int8\n",
    "\n",
    "tflite_model_int8 = converter.convert()\n",
    "\n",
    "with open(\"models/model_basic_int8.tflite\", \"wb\") as f:\n",
    "    f.write(tflite_model_int8)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare file sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model File                                Size (KB)  Size (MB)\n",
      "------------------------------------------------------------\n",
      "model_basic.keras                          56628.02      55.30\n",
      "model_basic_default.tflite                 18865.16      18.42\n",
      "model_basic_weight_quant.tflite             4726.91       4.62\n",
      "model_basic_fp16.tflite                     9435.71       9.21\n",
      "model_basic_int8.tflite                     4729.12       4.62\n"
     ]
    }
   ],
   "source": [
    "# Directory where all models are stored\n",
    "model_dir = \"models\"\n",
    "\n",
    "# List of model filenames\n",
    "model_files = [\n",
    "    \"model_basic.keras\",\n",
    "    \"model_basic_default.tflite\",\n",
    "    \"model_basic_weight_quant.tflite\",\n",
    "    \"model_basic_fp16.tflite\",\n",
    "    \"model_basic_int8.tflite\"\n",
    "]\n",
    "\n",
    "# Print file sizes in KB and MB\n",
    "print(f\"{'Model File':<40} {'Size (KB)':>10} {'Size (MB)':>10}\")\n",
    "print(\"-\" * 60)\n",
    "for filename in model_files:\n",
    "    filepath = os.path.join(model_dir, filename)\n",
    "    if os.path.exists(filepath):\n",
    "        size_bytes = os.path.getsize(filepath)\n",
    "        size_kb = size_bytes / 1024\n",
    "        size_mb = size_kb / 1024\n",
    "        print(f\"{filename:<40} {size_kb:10.2f} {size_mb:10.2f}\")\n",
    "    else:\n",
    "        print(f\"{filename:<40} {'Not Found':>10}\")\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNlr1VUpFOL+KGoqzjBbIab",
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": [
    {
     "file_id": "1oalBGiBEmDZvxE233V7M-RnEx_zLBCo2",
     "timestamp": 1731916991491
    },
    {
     "file_id": "1TidBHrj6S6jibGgIVOrAqboqCoymOKpT",
     "timestamp": 1731247492856
    },
    {
     "file_id": "14itFABlTmXZQ1eR6BhkZLGAU0FgD6vjk",
     "timestamp": 1731246687780
    },
    {
     "file_id": "1PDRZStVW0Kxup2Gjd3Io_IIFLfrwi1PF",
     "timestamp": 1731246298933
    },
    {
     "file_id": "1G02UBBG8iWQexT1R8eM_8lhi2sPhQW0w",
     "timestamp": 1731245349914
    },
    {
     "file_id": "1Ep3s7RgwDMqOwEin9Me7cXIFzvvwNrw0",
     "timestamp": 1727171988133
    },
    {
     "file_id": "1yNxezESUf3tpAYF5jEVXDVwP_W6VO0Bn",
     "timestamp": 1727171696041
    },
    {
     "file_id": "1mvCv1EcEUV2wpH-_6XMZst0eBwgShLdd",
     "timestamp": 1727171402872
    },
    {
     "file_id": "1wAI52GMhyM46NgLbdKNq6Z1rHtUr9N7-",
     "timestamp": 1727171247285
    },
    {
     "file_id": "1Vy-6Edy7EhINQff2nD8QfM56H8Fbem0C",
     "timestamp": 1727170846512
    },
    {
     "file_id": "15H-1Vzh-ucxzcXdWVBLevk7qJnVn9rXz",
     "timestamp": 1727170637970
    },
    {
     "file_id": "1P33goZRsjvsQIZGQt0l8n00RKG-jeyhM",
     "timestamp": 1727170514514
    },
    {
     "file_id": "190LRciNn_ryP9reQAEpEkiwsRSr3Qefo",
     "timestamp": 1727170379723
    },
    {
     "file_id": "18ZVvBSnI2yNO3ui3TSxzd0dAtqWweRLO",
     "timestamp": 1727170069420
    },
    {
     "file_id": "1CwUObcnsbf3a-1_F30kksyppa1C6HBsM",
     "timestamp": 1727169831345
    },
    {
     "file_id": "1N_UemmJCYGMux2-0g8WvwRrwyL9hDd9S",
     "timestamp": 1727167315641
    },
    {
     "file_id": "1ImETO8ub61dtz1N26Q4YzdugZxrqVeR8",
     "timestamp": 1727167195642
    },
    {
     "file_id": "1ZZEdd2EWS3m_-RV0OYDHek1KwGNUTB3V",
     "timestamp": 1727166316493
    },
    {
     "file_id": "1gWV9GLK4O09_BqMpfmimVXkBiOrc7G_Z",
     "timestamp": 1727165799781
    },
    {
     "file_id": "14b9-Fg2l9z013vOHG67g38maASX8OvPM",
     "timestamp": 1727164865047
    },
    {
     "file_id": "1UeMK5bToLS86K8fp-X1Eq3pP3MC3KzST",
     "timestamp": 1727163104132
    },
    {
     "file_id": "1T8H31Gsv4WUXxlu1XzJq8BPn6NpeMYBl",
     "timestamp": 1727162085575
    },
    {
     "file_id": "1kOd6rSw56P15kivDBbfFx7u82MFLL1HF",
     "timestamp": 1727161716778
    },
    {
     "file_id": "1WOL9v3Vzqm7sg1DCGlhKx9SUwy_rl-Un",
     "timestamp": 1727161666718
    },
    {
     "file_id": "1iMwCnoJBdqCjdZQD_0zAYnfT3IsiPs3r",
     "timestamp": 1727161625177
    },
    {
     "file_id": "17yR0txaYvnkEelcoCrcFfHyk2woB9-bo",
     "timestamp": 1727161265705
    },
    {
     "file_id": "1k1-xmLGVR_dbI0Ar2s8pPtuNgZvGAX8h",
     "timestamp": 1727160985075
    },
    {
     "file_id": "1upRTmE7k3lqqvVcvRIB7a2P_wDd_-Kjd",
     "timestamp": 1727160702898
    },
    {
     "file_id": "1cFXn3KLVv1RzZYu4LFnnGzDx6Ce7KCD-",
     "timestamp": 1727159722418
    }
   ]
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
